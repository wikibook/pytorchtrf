{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3756f-93ef-4d7b-a61c-70e60a689760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class VanillaSkipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embedding_dim,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        output = self.linear(embeddings)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452e944-99e4-4584-b6a0-88ddc77cf953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "corpus = pd.DataFrame(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f0eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Okt()\n",
    "tokens = [tokenizer.morphs(review) for review in corpus.text]\n",
    "print(tokens[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6242cddc-b96e-4cc3-8b14-6dd61a1e48b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "vocab = build_vocab(corpus=tokens, n_vocab=5000, special_tokens=[\"<unk>\"])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5be65c-a2c7-4a0a-b260-c69c5e27bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pairs(tokens, window_size):\n",
    "    pairs = []\n",
    "    for sentence in tokens:\n",
    "        sentence_length = len(sentence)\n",
    "        for idx, center_word in enumerate(sentence):\n",
    "            window_start = max(0, idx - window_size)\n",
    "            window_end = min(sentence_length, idx + window_size + 1)\n",
    "            center_word = sentence[idx]\n",
    "            context_words = sentence[window_start:idx] + sentence[idx+1:window_end]\n",
    "            for context_word in context_words:\n",
    "                pairs.append([center_word, context_word])\n",
    "    return pairs\n",
    "\n",
    "\n",
    "word_pairs = get_word_pairs(tokens, window_size=2)\n",
    "print(word_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3bb47-37b8-456a-8371-d1faaa4eda1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_pairs(word_pairs, token_to_id):\n",
    "    pairs = []\n",
    "    unk_index = token_to_id[\"<unk>\"]\n",
    "    for word_pair in word_pairs:\n",
    "        center_word, context_word = word_pair\n",
    "        center_index = token_to_id.get(center_word, unk_index)\n",
    "        context_index = token_to_id.get(context_word, unk_index)\n",
    "        pairs.append([center_index, context_index])\n",
    "    return pairs\n",
    "\n",
    "\n",
    "index_pairs = get_index_pairs(word_pairs, token_to_id)\n",
    "print(index_pairs[:5])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915d51c-6440-47e5-af3b-40c57923fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "index_pairs = torch.tensor(index_pairs)\n",
    "center_indexs = index_pairs[:, 0]\n",
    "contenxt_indexs = index_pairs[:, 1]\n",
    "\n",
    "dataset = TensorDataset(center_indexs, contenxt_indexs)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa634889-1672-48d4-b0d8-a8758f03d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "word2vec = VanillaSkipgram(vocab_size=len(token_to_id), embedding_dim=128).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(word2vec.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ab10e-517c-497f-99c3-7ad5058f2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    cost = 0.0\n",
    "    for input_ids, target_ids in dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "\n",
    "        logits = word2vec(input_ids)\n",
    "        loss = criterion(logits, target_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cost += loss\n",
    "\n",
    "    cost = cost / len(dataloader)\n",
    "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee517a-ea9f-4feb-8eac-b57a28a3db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_embedding = dict()\n",
    "embedding_matrix = word2vec.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, embedding in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word] = embedding\n",
    "\n",
    "index = 30\n",
    "token = vocab[30]\n",
    "token_embedding = token_to_embedding[token]\n",
    "print(token)\n",
    "print(token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea695e-8c70-4736-a427-b61322011344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    cosine = np.dot(b, a) / (norm(b, axis=1) * norm(a))\n",
    "    return cosine\n",
    "\n",
    "def top_n_index(cosine_matrix, n):\n",
    "    closest_indexes = cosine_matrix.argsort()[::-1]\n",
    "    top_n = closest_indexes[1 : n + 1]\n",
    "    return top_n\n",
    "\n",
    "\n",
    "cosine_matrix = cosine_similarity(token_embedding, embedding_matrix)\n",
    "top_n = top_n_index(cosine_matrix, n=5)\n",
    "\n",
    "print(f\"{token}와 가장 유사한 5 개 단어\")\n",
    "for index in top_n:\n",
    "    print(f\"{id_to_token[index]} - 유사도 : {cosine_matrix[index]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
