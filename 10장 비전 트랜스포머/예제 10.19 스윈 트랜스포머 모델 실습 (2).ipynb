{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9324f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 애플 실리콘이 탑재된 맥 사용자의 경우 해당 설정으로 변경한다.\n",
    "# import os\n",
    "# os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e00d6d-5009-4113-b19c-a6aae119bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from transformers import AutoImageProcessor\n",
    "from transformers import SwinForImageClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "def subset_sampler(dataset, classes, max_len):\n",
    "    target_idx = defaultdict(list)\n",
    "    for idx, label in enumerate(dataset.train_labels):\n",
    "        target_idx[int(label)].append(idx)\n",
    "\n",
    "    indices = list(\n",
    "        chain.from_iterable(\n",
    "            [target_idx[idx][:max_len] for idx in range(len(classes))]\n",
    "        )\n",
    "    )\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "\n",
    "def model_init(classes, class_to_idx):\n",
    "    model = SwinForImageClassification.from_pretrained(\n",
    "        pretrained_model_name_or_path=\"microsoft/swin-tiny-patch4-window7-224\",\n",
    "        num_labels=len(classes),\n",
    "        id2label={idx: label for label, idx in class_to_idx.items()},\n",
    "        label2id=class_to_idx,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def collator(data, transform):\n",
    "    images, labels = zip(*data)\n",
    "    pixel_values = torch.stack([transform(image) for image in images])\n",
    "    labels = torch.tensor([label for label in labels])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = evaluate.load(\"f1\")\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    macro_f1 = metric.compute(\n",
    "        predictions=predictions, references=labels, average=\"macro\"\n",
    "    )\n",
    "    return macro_f1\n",
    "\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=True)\n",
    "test_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=False)\n",
    "\n",
    "classes = train_dataset.classes\n",
    "class_to_idx = train_dataset.class_to_idx\n",
    "\n",
    "subset_train_dataset = subset_sampler(\n",
    "    dataset=train_dataset, classes=train_dataset.classes, max_len=1000\n",
    ")\n",
    "subset_test_dataset = subset_sampler(\n",
    "    dataset=test_dataset, classes=test_dataset.classes, max_len=100\n",
    ")\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"microsoft/swin-tiny-patch4-window7-224\"\n",
    ")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(\n",
    "            size=(\n",
    "                image_processor.size[\"height\"],\n",
    "                image_processor.size[\"width\"]\n",
    "            )\n",
    "        ),\n",
    "        transforms.Lambda(\n",
    "            lambda x: torch.cat([x, x, x], 0)\n",
    "        ),\n",
    "        transforms.Normalize(\n",
    "            mean=image_processor.image_mean,\n",
    "            std=image_processor.image_std\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../models/Swin-FashionMNIST\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.001,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=125,\n",
    "    remove_unused_columns=False,\n",
    "    seed=7\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=lambda x: model_init(classes, class_to_idx),\n",
    "    args=args,\n",
    "    train_dataset=subset_train_dataset,\n",
    "    eval_dataset=subset_test_dataset,\n",
    "    data_collator=lambda x: collator(x, transform),\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=image_processor,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6321b3a8-3aa3-4b76-b447-9547d056705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "outputs = trainer.predict(subset_test_dataset)\n",
    "print(outputs)\n",
    "\n",
    "y_true = outputs.label_ids\n",
    "y_pred = outputs.predictions.argmax(1)\n",
    "\n",
    "labels = list(classes)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=labels)\n",
    "_, ax = plt.subplots(figsize=(10, 10))\n",
    "display.plot(xticks_rotation=45, ax=ax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
